Inspiration
The current influx of Corona-related fake news is unprecedented in terms of how widespread such misleading or malicious content is being shared (geographically) and in terms of the real harm it is causing, right now, to real people and communities all over the world.

Currently, companies such as Facebook, Google and Twitter rely mainly on in-house fact checkers, sometimes in cooperation with external independently accredited fact-checking organisations, to check content for fake or misleading news. The problem is:

these teams can't cope with the huge influx of fake news - meaning a lot of content is gaining traction before being flagged
these teams are typically based in only a handful of countries and can only check content in a limited number of languages - meaning that the effort is currently concentrated in the most developed nations, and Corona-related fake news is running rampant in Africa and much of Latin America and Asia.
At the same time, millions of people are currently stuck at home in lock down, itching at any opportunity to help out.

This article goes into more depth on what the issue is, and how a community-driven platform could help resolve it.

What Problems does the Platform Solve?
A public, community-driven platform for identifying fake news would:

allow us to vastly expand efforts / personnel available for identifying fake news, thereby ensuring that such content is flagged quickly, before gaining widespread traction;
ensure that this effort is carried out worldwide and in all languages, rather than being concentrated in the most developed nations.
How We Envision the Platform Working
Users sign up with a social media or google profile that has been active for a month or more (to avoid trolls). Upon signing up, participants indicate in which languages they are willing to review online content.

Users are presented with a short video introduction to the platform, and the various designations an article can receive (see below). After watching the video, the user fills out a short questionnaire to show that they have understood the designation system.

After passing the questionnaire, users are presented with their first online content to review. To avoid driving extra traffic to fake news websites, the content is not linked, but automatically copy-pasted from the original website and hosted on a designated server. Insofar possible, any content which may indicate to the user which news source or platform the continent originates from (such as website banners), is removed in the version the user sees.

After reviewing the content, the user can then rate the content using a limited number of designations, such: "real news", "misleading", "suspicious", "fake news", "conspiracy theory", "parody", etc. Not quite as simple as Tinder's like or dislike, but not so complicated that users actually have to write a full explanation of why they have picked the designation they picked.

The intention is not to "democratically decide" what content should or shouldn't be considered as fake news, as this would potentially lead to bias. Rather, the intention is for the platform to serve as an amplification / extension of the work already being carried out by professional fact checkers at for instance Facebook, Google and Twitter. As such, only users whose judgement is consistently in line with that of professional fact checkers, will have their judgements count in the final algorithm deciding what designation the online content should receive. This would work as follows: a user's input is only taken into consideration once they have submitted at least 5 reviews of online content. Unbeknownst to them, 3 out of the 5 articles they reviewed have already been reviewed by professional fact checkers. Based on whether or not the user's judgement correlates with that of the professional fact checkers, the user receives a "reliability score" from 0 to 1. Content judgements from users with a high reliability score receive a higher weight in the final algorithm, whereas judgements from users with a low reliability score receive a lower weight, or are disregarded. As users continue to use the platform, 1 out of 5 articles (randomised) they review are articles which have already been reviewed by professional fact checkers, so that the user's reliability score is continuously updated.

Once certain online content has been reviewed by X number of users with a high reliability score (so not professional fact checkers, but people who make judgements in line with professional fact checkers) who have given it the same designation, the algorithm considers this designation reliable, and the content receives its final designation. Depending on the designation it received, companies such as Facebook, Google and Twitter can then take appropriate action to either promote the content or limit its spread.

The whole process should be accelerated by machine learning (again, using the algorithms already in place at companies such as Facebook, Google, Twitter, ...): identifying which online content is gaining most traction, selecting which articles should most urgently be reviewed by participants (in case of the current crisis, mainly content related to the coronavirus), automatically identifying fake news content which has been copy pasted or duplicated with minimal adjustments across different platforms, etc.

As a possible extension of the platform, allow users themselves to submit links to online which they have themselves spotted and identified as fake news.

Gamify the platform: allow users to clearly see how many articles they've reviewed, how many of their reviews have resulted in the algorithm identifying content as fake news, create leaderboards, award users badges (indicating for instance how many articles they've reviewed, whether or not their judgement is in line with professional fact checkers, how many articles they have themselves "discovered" which have been subsequently identified as fake news by the platform), etc.

What Do We Want to Produce by the End of the Hackathon?
The intention is not to create a fully functional platform by the end of next weekend. The final platform would have to rely on the input of professional fact checkers, as well as machine learning algorithms to decide which content should be presented to users.

Instead the intention is to create a good "concept design", which engineers at Facebook, Google, Twitter, .. can then use as a blueprint to which to hook up their teams of professional fact checkers / the machine learning algorithms they already have in place.

Things to brainstorm / accomplish by the end of the weekend:

What functionality should the front-end have? => develop a proof of concept website
What does the back-end pipeline look like conceptually? => create a blueprint for the pipeline
Who Are We Looking For?
For now this is just an idea we're putting out there - we don't have a team assembled. If you think this is an idea worth pursuing, and want to help us think it through, come on in! We need a wide range of people:

A team lead with hackathon experience
Interface / website developers
Graphic designers
Anyone with relevant insights in fake news, machine learning, crowd wisdom, ...
Anyone wishing to brainstorm this further and willing to help us spot / avoid potential pitfalls
Main Critiques (and Responses) to a Community-Driven Approach for Identifying Fake News
A. Political or ideological bias will pervade a community-driven response if community members collectively lean one way or the other on a political or ideological spectrum.

=> Response 1: Correlating user-input with professional fact checkers would ensure that a community-driven approach is no more, or less, biased than the current approach.

=> Response 2: Corona-related fake news is less a matter of political or ideological preference than other types of fake news. It is a matter of overcoming this crisis together. Whether we lean left or right, live in the developed or the developing world, for once we are all in the same boat, facing a common enemy.

B. Fake news is produced so abundantly, and spreads so rapidly to different platforms that it is impossible to curb.

=> Response: Producing believable fake news does in fact require quite a lot of creative thinking and time. Fake news is being spread by a relatively small number of individuals, and the means at their disposal for spreading such content are relatively basic. Fake news content is spread to other platforms simply by copy pasting content, or by reproducing it with minimal alterations. The "bad side" thus has a relatively labour-intensive task at hand, and has relatively unsophisticated means for spreading fake news. Meanwhile, the number of "good people" capable of recognising fake news vastly outnumber the people creating fake news, and identifying (often glaringly obvious) fake news is a lot less labour intensive than producing it. As such, if the Internet Giants throw their capital and resources (such as machine learning algorithms) behind a community-driven approach to identifying fake news, the means that the "good side" has available for combating fake news are vastly greater and more sophisticated than the resources the "bad side" has available. By all measures, this should be an unfair fight, with the advantage being for the "good side".

C. Who are we to decide what users should or shouldn't view? What about free speech?

=> Response 1: We currently already accept that professional fact checkers can to some extent decide what content should be promoted, and what content should be limited. It is not unethical for companies such as Facebook, Google or Twitter to take the stance that they wish to promote reliable news, and limit the spread of fake or harmful corona-related fake news.

=> Response 2: A community-driven approach does not have to be black or white: there are many gradations between obviously malicious and harmful fake news, and completely reliable news. Other designations such as "conspiracy theory", "parody", "misleading news", ... can be used, with different response strategies being appropriate for each category.